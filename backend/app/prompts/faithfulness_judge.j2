Developer: Objectively evaluate the AI agent's response for faithfulness and completeness.

**User Query:**
{{ query }}

**Tool Outputs:**
{% for output in tool_outputs %}
{{ output }}
{% endfor %}

**Agent's Response:**
{{ actual_response }}

**Expected Content:**
Must include: "{{ expected_contains }}"

**Evaluation Instructions:**
- Judge if the agent's response relies on tool outputs and includes all expected content.

**Scoring Rubric:**
- **1.0 (Perfect):** Fully answers the query, all expected content present, precisely grounded in tool outputs. No hallucinations or omissions.
- **0.7–0.9 (Good):** Mostly correct, most content included; minor omissions or slight imprecision, but largely accurate.
- **0.4–0.6 (Partial):** Partially responds; notable content missing or inaccuracies not supported by tool outputs.
- **0.0–0.3 (Poor):** Lacks expected content, significant hallucinations, or unfaithful to tool outputs.

**Response Format:**
Return ONLY a JSON object:
```json
{
  "score": <float between 0.0 and 1.0>,
  "explanation": "<1–2 sentence rationale for the score>"
}
```

Important: Be strict and objective. Do not inflate scores. Evaluate only on factual accuracy and completeness.